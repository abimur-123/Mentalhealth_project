high = "green",
breaks = seq(0,2000, 500),
labels = paste(seq(0,2000, 500), "hits"),
guide = guide_legend(title="hits"),
trans = 'reverse') +
labs(title = "Runs vs Walks vs Hits")
paste(exp(2.444e-04),exp(7.954e-04))
paste(exp(2.444e-04),exp(2.444e-04),exp(7.954e-04))
paste(exp(5.321),exp(2.444e-04),exp(7.954e-04))
paste("Intercept",exp(5.321),
"Walks",exp(2.444e-04),
"Hits",exp(7.954e-04))
log_glm_int <- glm(Survived~Sex*Fare,data = dat %>% filter(Sex == "female"),family = binomial(link = "logit"))
log_glm_int <- glm(Survived~Sex*Fare,data = dat,family = binomial(link = "logit"))
log_glm_int
summary(log_glm_int)
dat %>%
ggplot(.,aes(x = Fare,y = Survived)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "lm") +
facet_grid(~ Sex) +
labs(title = "LM model on the Survived against Fare and Sex")
dat %>%
ggplot(.,aes(x = Fare,y = Survived)) +
geom_jitter(alpha = 0.25) +
geom_smooth(method = "glm",method.args = list(family = binomial(link = 'logit'))) +
facet_wrap(~Sex) +
labs(title = "Binomial regression(logit) to predict Survival")
dat %>%
ggplot(.,aes(x = Fare,y = Survived)) +
geom_jitter(alpha = 0.25) +
geom_smooth(method = "glm",method.args = list(family = binomial(link = 'probit'))) +
facet_wrap(~Sex) +
labs(title = "Binomial regression(probit)")
knitr::opts_chunk$set(echo = TRUE)
indicator
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(cluster)
library(tidyr)
library(mclust)
library(factoextra)
library(mvtnorm)
x <- rnorm(10000,0,1)
y <- rnorm(10000,5,2)
z = x + y
ggplot() +
aes(z) +
geom_density() +
geom_vline(xintercept = mean(z)) +
labs(title = "Z =  X + Y") +
geom_text(aes(x = mean(z), y = 0.05, label = paste("Mean",round(mean(z),2))),vjust = -1,angle = 90)
c(paste("mean:",mean(z)),paste("variance",var(z)))
#src: https://stats.stackexchange.com/a/70881
mgmm <- function(components,samples,size,mean,sd){
N <- samples #size
components <- sample(1:components,prob=size,size=N,replace=TRUE) #size of each component
mus <- mean #means of each component
sds <- sd #sd of each component
samples <- rnorm(n=N,mean=mus[components],sd=sds[components])
samples
}
Z <- mgmm(2,10000,size = c(0.2,0.8),c(0,5),c(1,2))
length(Z)
ggplot() +
aes(Z) +
geom_density() +
geom_vline(xintercept = mean(Z)) +
labs(title = "Gaussian mixture") +
geom_text(aes(x = mean(Z), y = 0.05, label = paste("Mean",round(mean(Z),2))),vjust = -1,angle = 90)
c(paste("mean:",mean(Z)),paste("variance",var(Z)))
data <- readRDS("data/clusterData.rds")$X
cluster::pam(data,4,metric = "euclidean")
Distance <- dist(data, method = "euclidean")
hie_euc <- hclust(Distance)
fviz_dend(hie_euc, k = 2 ,main = "Cluster dendogram - Euclidean Distance",rect = TRUE,show_labels = FALSE,rect_border = "black")
ggplot() +
aes(x = data[,1], y = data[,2],color = as.factor(cutree(hie_euc, k = 2, h = NULL))) +
geom_point() +
labs(color = "Cluster",
title = "2 cluster cut"
)
res <-  pam(Distance,4)
fviz_silhouette(res)
res <-  pam(Distance,2)
fviz_silhouette(res)
data <- readRDS("data/clusterData.rds")$X
dat <- scale(as.matrix(data))
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
em <- function(dat){
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
d <-  2 #dimensionality
k <-  3 #clusters
mus <-  c(list(mu1),list(mu2),list(mu3))
s <- c(list(diag(var1)),list(diag(var2)),list(diag(var3)))
phis <- c(0.4, 0.2, 0.4)
n <- nrow(dat)
gamma <- matrix(0, n, k)
#Expecation
gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(dat,mean=mus[[k]],sigma=s[[k]])})
tmp <- rowSums(gamma)
gamma <- gamma / tmp
loglik <-  sum(log(tmp))
#Maximization
pi  <-  colSums(gamma)
# mu <- gamma[,1:K] * dat
# mu <- apply(mu,2,sum)
# mu  <-  mu / pi[1:K]
mus <- lapply(1:k,function(k) {colSums(dat * gamma[,k ]) / sum(gamma[, k])} )
pi  <-  pi / n
s <- lapply(1:k,FUN = function(k){ colSums(gamma[, k] * sweep(dat, 2, mus[[k]], `-`) ^ 2)/ sum(gamma[, k]) })
return(list(rij = gamma,mean_values = mus,variance = s,weights = pi))
}
em_values <- em(data)[-1]
mus <- em(data)$mean_values
gamma <- em(data)$rij
em_values
k <- 3
n <- nrow(data)
Em <- em(data)
mu <- Em$mean_values
sigmas <- Em$variance
phis <- Em$weights
sds <- lapply(sigmas, function(x) sqrt(x))
# Generate Mixed Gaussian
N <- 1500
new_vals <-  data.frame(X = numeric(N), Y = numeric(N))
for(i in (1:N)){
## generate random value between 0 and 1 to compare to phi(pi) values
val <- runif(1)
if(val < phis[1]){
new_vals[i,] <- rmvnorm(1,mu[[1]],diag(sigmas[[1]]))
}
else if(val < (phis[1] + phis[2])){
new_vals[i,] <- rmvnorm(1,mu[[2]],diag(sigmas[[2]]))
}
else{
new_vals[i,] <- rmvnorm(1,mu[[3]],diag(sigmas[[3]]))
}
}
# estimated groups
emlab <- apply(em(data)$rij, 1, which.max) #argmax in Python
df <- cbind(as.data.frame(data),emlab)
colnames(df) <- c("X","Y","Cluster")
ggplot(df,aes(X,Y)) +
geom_point(aes(color = as.factor(Cluster))) +
geom_density2d(data = new_vals,aes(X,Y)) +
labs(title = "Hard assigment of clusters",
color = "Cluster"
)
Em <- em(data)
mu <- Em$mean_values
sigmas <- Em$variance
phis <- Em$weights
sds <- lapply(sigmas, function(x) sqrt(x))
new_gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(data,mean=mu[[k]],sigma=diag(sigmas[[k]]))})
tmp <- rowSums(new_gamma)
new_gamma <- new_gamma / tmp
indicator <- sapply(1:k,function(k) {apply(new_gamma, 1, function(x) which.max(x)) == k})
AIC <- 2 * 3 - 2 * sum(log(colSums(indicator)))
paste("AIC",AIC)
indicator
new_gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(data,mean=mu[[k]],sigma=diag(sigmas[[k]]))})
tmp <- rowSums(new_gamma)
new_gamma <- new_gamma / tmp
indicator <- sapply(1:k,function(k) {apply(new_gamma, 1, function(x) which.max(x)) == k})
AIC <- 2 * 3 - 2 * sum(log(colSums(indicator)))
paste("AIC",AIC)
mc <- Mclust(data)
# BIC values used for choosing the number of clusters
fviz_mclust(mc, "BIC", palette = "jco")
mc <- Mclust(scale(data), G = 4, modelNames = "EEV")
fviz_mclust(mc, "classification", ellipse.type = c("norm"),geom = "point",
pointsize = 1.5, palette = "jco",xlab = "X",ylab = "Y") + geom_density_2d()
summary(mc)
UN <- readRDS("data/subvotes.rds")
Country_df <- UN %>%
spread(country, vote)
Country_mat <- as.matrix(Country_df[,-1])
# rownames(Country_mat) <- Country_df$rcid
Country_mat_na <- apply(Country_mat, 2, function(x){
x[is.na(x)] <- as.numeric(names(which.max(table(x))))
return(x) })
m <- Mclust(dist(t(Country_mat_na)),G = 1:10)
fviz_mclust(m, "classification", geom = "point",
pointsize = 1.5, palette = "jco")
data <- readRDS("data/clusterData.rds")$X
# dat <- scale(as.matrix(data))
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
d <-  2 #dimensionality
k <-  3 #clusters
em <- function(dat){
## This function implements the EM algorithm
## Input : data - data matrix
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
d <-  2 #dimensionality
k <-  3 #clusters
mus <-  c(list(mu1),list(mu2),list(mu3))
s <- c(list(diag(var1)),list(diag(var2)),list(diag(var3)))
phis <- c(0.4, 0.2, 0.4)
n <- nrow(dat)
gamma <- matrix(0, n, k)
#Expecation
gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(dat,mean=mus[[k]],sigma=s[[k]])})
tmp <- rowSums(gamma)
gamma <- gamma / tmp
loglik <-  sum(log(tmp))
#Maximization
pi  <-  colSums(gamma)
# mu <- gamma[,1:K] * dat
# mu <- apply(mu,2,sum)
# mu  <-  mu / pi[1:K]
mus <- lapply(1:k,function(k) {colSums(dat * gamma[,k ]) / sum(gamma[, k])} )
pi  <-  pi / n
s <- lapply(1:k,FUN = function(k){ colSums(gamma[, k] * sweep(dat, 2, mus[[k]], `-`) ^ 2)/ sum(gamma[, k]) })
return(list(rij = gamma,mean_values = mus,variance = s,weights = pi))
}
em_values <- em(data)[-1]
mus <- em(data)$mean_values
gamma <- em(data)$rij
em_values <- em(data)[-1]
mus <- em(data)$mean_values
gamma <- em(data)$rij
em_values
k <- 3
n <- nrow(data)
Em <- em(data)
mu <- Em$mean_values
sigmas <- Em$variance
phis <- Em$weights
sds <- lapply(sigmas, function(x) sqrt(x))
# Generate Mixed Gaussian
N <- 1500
new_vals <-  data.frame(X = numeric(N), Y = numeric(N))
for(i in (1:N)){
## generate random value between 0 and 1 to compare to phi(pi) values
val <- runif(1)
if(val < phis[1]){
new_vals[i,] <- rmvnorm(1,mu[[1]],diag(sigmas[[1]]))
}
else if(val < (phis[1] + phis[2])){
new_vals[i,] <- rmvnorm(1,mu[[2]],diag(sigmas[[2]]))
}
else{
new_vals[i,] <- rmvnorm(1,mu[[3]],diag(sigmas[[3]]))
}
}
# estimated groups
emlab <- apply(em(data)$rij, 1, which.max) #argmax in Python
df <- cbind(as.data.frame(data),emlab)
colnames(df) <- c("X","Y","Cluster")
ggplot(df,aes(X,Y)) +
geom_point(aes(color = as.factor(Cluster))) +
geom_density2d(data = new_vals,aes(X,Y)) +
labs(title = "Hard assigment of clusters",
color = "Cluster"
)
Em <- em(data)
mu <- Em$mean_values
sigmas <- Em$variance
phis <- Em$weights
new_gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(data,mean=mu[[k]],sigma=diag(sigmas[[k]]))})
tmp <- rowSums(new_gamma)
new_gamma <- new_gamma / tmp
indicator <- sapply(1:k,function(k) {apply(new_gamma, 1, function(x) which.max(x)) == k})
AIC <- 2 * 3 - 2 * sum(log(colSums(indicator)))
paste("AIC",AIC)
em <- function(dat){
## This function implements the EM algorithm
## Input : data - data matrix
mu1 <- c(-10, -8)  # Mean of cluster 1
var1 <- c(3, 2)    # Variances of cluster 1
mu2 <- c(12, -4)   # Mean of cluster 2
var2 <- c(4, 2.5)  # ...etc...
mu3 <- c(4, 18)
var3 <- c(42, 8.5)
phis <- c(0.4, 0.2, 0.4) # Mixture probabilities.(or size of cluster)
d <-  2 #dimensionality
k <-  3 #clusters
#list of list
mus <-  c(list(mu1),list(mu2),list(mu3))
s <- c(list(diag(var1)),list(diag(var2)),list(diag(var3)))
phis <- c(0.4, 0.2, 0.4)
n <- nrow(dat)
gamma <- matrix(0, n, k) #responsibility matrix
#Expecation
gamma <-  sapply(1:k,function(k){ phis[k] * dmvnorm(dat,mean=mus[[k]],sigma=s[[k]])})
tmp <- rowSums(gamma)
gamma <- gamma / tmp
loglik <-  sum(log(tmp))
#Maximization
pi  <-  colSums(gamma)
mus <- lapply(1:k,function(k) {colSums(dat * gamma[,k ]) / sum(gamma[, k])} )
pi  <-  pi / n
s <- lapply(1:k,FUN = function(k){ colSums(gamma[, k] * sweep(dat, 2, mus[[k]], `-`) ^ 2)/ sum(gamma[, k]) })
return(list(rij = gamma,mean_values = mus,variance = s,weights = pi))
}
em_values <- em(data)[-1]
mus <- em(data)$mean_values
gamma <- em(data)$rij
em_values
mc <- Mclust(data)
# BIC values used for choosing the number of clusters
fviz_mclust(mc, "BIC", palette = "jco")
mc <- Mclust(scale(data), G = 4, modelNames = "EEV")
fviz_mclust(mc, "classification", ellipse.type = c("norm"),geom = "point",
pointsize = 1.5, palette = "jco",xlab = "X",ylab = "Y") + geom_density_2d()
summary(mc)
UN <- readRDS("data/subvotes.rds")
Country_df <- UN %>%
spread(country, vote)
Country_mat <- as.matrix(Country_df[,-1])
# rownames(Country_mat) <- Country_df$rcid
Country_mat_na <- apply(Country_mat, 2, function(x){
x[is.na(x)] <- as.numeric(names(which.max(table(x))))
return(x) })
m <- Mclust(dist(t(Country_mat_na)),G = 1:10)
fviz_mclust(m, "classification", geom = "point",
pointsize = 1.5, palette = "jco")
library(nlme)
library(Lahman)
library(dplyr)
library(lme4)
head(Teams)
Teams_df <- Teams %>%
select(runs = R,walks = BB,hits = H,team = teamID)
lmer(runs ~ scale(hits) + (scale(hits) | team) + scale(walks) + (scale(walks) | team), data = Teams_df)
lmer(I(runs ~ scale(hits) + (scale(hits) | team) + scale(walks) + (scale(walks) | team)), data = Teams_df)
Teams_df
lmer(I(runs ~ scale(hits) + scale(walks) + (scale(walks) | team) + (scale(hits) | team) ), data = Teams_df)
lmer(I(runs ~ scale(hits) + scale(walks) + (scale(hits) | team) + (scale(walks) | team) ), data = Teams_df)
Teams_df <- Teams %>% filter(year > = 1990)
Teams_df <- Teams %>% filter(year >= 1990)
Teams_df <- Teams %>% filter(year >= 1990) %>%
select(runs = R,walks = BB,hits = H,team = teamID)
Teams
Teams_df <- Teams %>% filter(yearID >= 1990) %>%
select(runs = R,walks = BB,hits = H,team = teamID)
lmer(I(runs ~ scale(hits) + scale(walks) + (scale(hits) | team) + (scale(walks) | team) ), data = Teams_df)
runs_lm <- lmer(I(runs ~ scale(hits) + scale(walks) + (scale(hits) | team) + (scale(walks) | team) ), data = Teams_df)
summary(runs_lm)
library(broom)
augment(runs_lm)
resid_df <- augment(runs_lm)
plot(resid_df$.resid)
Teams_df %>% group_by(team) %>%
lm(runs ~ hits + walks)
apply(Teams_df,function(x) lm(x$runs ~ x$hits + x$walks))
apply(Teams_df,1,function(x) lm(x$runs ~ x$hits + x$walks))
ggplot(runs_lm,aes(.resid,.fitted)) +
geom_point()
library(nlme)
library(Lahman)
library(dplyr)
library(lme4)
library(broom)
library(ggplot2)
head(Teams)
ggplot(runs_lm,aes(.resid,.fitted)) +
geom_point()
resid_df <- augment(runs_lm)
ggplot(runs_lm,aes(y = .resid,x = .fitted)) +
geom_point()
apply(Teams_df,1,function(x) lm(x["runs"] ~ x$["hits"]))
apply(Teams_df,1,function(x) print(x))
apply(Teams_df,1,function(x) print(x[1]))
apply(Teams_df,1,function(x) print(x[runs]))
apply(Teams_df,1,function(x) print(x["runs"]))
lm(x["runs"] ~ x["walks"])
apply(Teams_df,1,function(x){
lm(x[1] ~ x[2])
}
)
apply(Teams_df,1,function(x){
lm(numeric(x[1]) ~ numeric(x[2])
}
apply(Teams_df,1,function(x){
lm(numeric(x[1]) ~ numeric(x[2]))
}
)
apply(Teams_df,1,function(x){
print(c(x[1], x[2]))
}
)
lapply(Teams_df,1,function(x){
print(c(x[1], x[2]))
}
)
lapply(Teams_df,function(x){
print(c(x[1], x[2]))
}
)
lapply(Teams_df,function(x){
lm(x[1] ~ x[2])
}
)
lapply(Teams_df,function(x){
print(x)
}
)
type(x[1])
lapply(Teams_df,function(x){
type(x[1])
}
)
apply(Teams_df,1,function(x){
type(x[1])
}
)
apply(Teams_df,1,function(x){
typeof(x[1])
}
)
str(Teams_df)
Teams_df %>%
purrr::map(
function(x) coef(lm(runs ~ walks))
)
str(Teams_df)
Teams_df %>%
purrr::map(
function(x) coef(lm(runs ~ walks, data = .))
)
Teams_df %>%
purrr::map(
function(x) coef(lm(Team_df$runs ~ walks))
)
Teams_df %>%
purrr::map(
function(x) coef(lm(Teams_df$runs ~ walks))
)
Teams_df %>%
purrr::map(
function(x) print(x)
)
Teams_df %>%
purrr::map(
function(x,y) print(c(x,y))
)
Teams_df %>%
purrr::map(
function(x) print(x[1])
)
Teams_df %>%
purrr::map(
function(x) print(x[2])
)
Teams_df %>%
purrr::map(
function(x) print(x[2]$runs)
)
Teams_df %>%
purrr::map(
function(x) print(x[2]["runs"])
)
fm1 <- lmList(runs ~ walks + hits, data = Teams_df)
fm1 <- lmList(runs ~ walks | team + hits | teams, data = Teams_df)
fm1 <- lmList(runs ~ walks | team + hits | team, data = Teams_df)
fm1 <- lmList(runs ~ walks | team, data = Teams_df)
fm1 <- lmList(runs ~ (walks + hits)| team, data = Teams_df)
summary(fm1)
ggplot(Teams_df,aes(x = walks, y = runs, colour = hits)) +
geom_point() +
geom_smooth() +
facet_grid(~team)
ggplot(Teams_df,aes(x = walks, y = runs, colour = hits)) +
geom_point() +
geom_smooth(method = "lm") +
facet_grid(~team)
Teams_df
ggplot(Teams_df %>% filter(team %in% ("NYA","MIN","PHI")),aes(x = walks, y = runs, colour = hits)) +
ggplot(Teams_df %>% filter(team %in% c("NYA","MIN","PHI")),aes(x = walks, y = runs, colour = hits)) +
geom_point() +
geom_smooth(method = "lm") +
facet_grid(~team)
install.packages("sjPlot")
shiny::runApp('Github_personal/Mentalhealth_project/Mentalhealth_exploration_Shiny')
shiny::runApp('Github_personal/Mentalhealth_project/Mentalhealth_exploration_Shiny')
runApp('Github_personal/Mentalhealth_project/Mentalhealth_exploration_Shiny')
runApp('Github_personal/Mentalhealth_project/Mentalhealth_exploration_Shiny')
